{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFqqstdPBnFE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILOIIv2mycxE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import hstack\n",
        "import spacy\n",
        "import string\n",
        "from scipy.sparse import csr_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kl6SszWysP8",
        "outputId": "32a1a9fb-803c-44f9-81ca-537893c3cad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7fdee16345c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "file_path = 'Dataset.csv'\n",
        "train_data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5rLPZslzErf",
        "outputId": "f5c340e6-448c-4196-ccec-6913e050db3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6443                             biographies\n",
            "5846                             biographies\n",
            "7104                             programming\n",
            "251                              biographies\n",
            "8606                             biographies\n",
            "                        ...                 \n",
            "5734    movies about artificial intelligence\n",
            "5191                              philosophy\n",
            "5390                             biographies\n",
            "860                              biographies\n",
            "7270                             programming\n",
            "Name: category, Length: 7477, dtype: object\n"
          ]
        }
      ],
      "source": [
        "train_data.replace(\"data missing\", np.nan, inplace=True)\n",
        "X = train_data.drop(columns=['category', 'par_id', 'lexicon_count', 'difficult_words', 'last_editor_gender', 'text_clarity'])\n",
        "y = train_data['category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Reshape y_train and y_test to a 2D array\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Fit the imputer on y_train\n",
        "imputer.fit(y_train)\n",
        "\n",
        "# Transform y_train and y_test\n",
        "y_train_imputed = imputer.transform(y_train)\n",
        "y_test_imputed = imputer.transform(y_test)\n",
        "\n",
        "# Flatten the arrays back to 1D\n",
        "y_train_imputed = y_train_imputed.flatten()\n",
        "y_test_imputed = y_test_imputed.flatten()\n",
        "\n",
        "# Convert back to pandas Series if needed\n",
        "y_train_imputed = pd.Series(y_train_imputed, name='category')\n",
        "y_test_imputed = pd.Series(y_test_imputed, name='category')\n"
      ],
      "metadata": {
        "id": "A1mcq30YGU1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_imputed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaYMDkVFGYYe",
        "outputId": "b67dc74e-1881-4c9a-b1bc-b4b8042345c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                biographies\n",
            "1                                biographies\n",
            "2                                programming\n",
            "3                                biographies\n",
            "4                                biographies\n",
            "                        ...                 \n",
            "7472    movies about artificial intelligence\n",
            "7473                              philosophy\n",
            "7474                             biographies\n",
            "7475                             biographies\n",
            "7476                             programming\n",
            "Name: category, Length: 7477, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_imputed.replace({\n",
        "    'artificial intelligence': 'Artificial intelligence',\n",
        "    'biography': 'Biography',\n",
        "    'movies about artificial intelligence': 'Movies about artificial intelligence',\n",
        "    'philosophy': 'Philosophy',\n",
        "    'programming': 'Programming'\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "MzWmDODvHEoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train_imputed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF7ECduHHHPx",
        "outputId": "038a27f7-6d9d-46c3-f6e6-cf803909eec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial intelligence' 'Biographies'\n",
            " 'Movies about artificial intelligence' 'Philosophy' 'Programming'\n",
            " 'biographies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RitFHR0M52Ko",
        "outputId": "75006bbd-ea48-4988-c6fa-7f27d941e73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "y_train_encoded = encoder.fit_transform(y_train_imputed.values.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test_imputed.values.reshape(-1, 1))\n",
        "print(y_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63Y9nDrSzItl"
      },
      "outputs": [],
      "source": [
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
        "\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G--I2dLzKtt",
        "outputId": "c60527d1-2c9f-4b20-81bd-8e48c3c3b807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           paragraph                      has_entity  \\\n",
            "0  Extension of the Bank to the north-west, the e...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "1  Thomson's separation of neon isotopes by their...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "2  The Python License was an open-source, GPL-com...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "3   Now let's be clear before we go any further t...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "4  A General System of Botany, Descriptive and An...   ORG_NO_PRODUCT_NO_PERSON_YES_   \n",
            "5  The Bahaʼi Faith asserts that evil is non-exis...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "6  In August 1993, it was discovered that the pro...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "7  Roko's basilisk has gained a significant amoun...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "8  1675 – Some Considerations about the Reconcile...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "9  For nearly 30 years Ehrenberg examined samples...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_NO_PERSON_YES_  has_entity_ORG_NO_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                       0.0                                       0.0   \n",
            "1                                       0.0                                       0.0   \n",
            "2                                       0.0                                       0.0   \n",
            "3                                       0.0                                       0.0   \n",
            "4                                       1.0                                       0.0   \n",
            "5                                       0.0                                       0.0   \n",
            "6                                       0.0                                       0.0   \n",
            "7                                       0.0                                       0.0   \n",
            "8                                       0.0                                       0.0   \n",
            "9                                       0.0                                       0.0   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_YES_PERSON_YES_  has_entity_ORG_YES_PRODUCT_NO_PERSON_NO_  \\\n",
            "0                                        0.0                                       1.0   \n",
            "1                                        0.0                                       0.0   \n",
            "2                                        0.0                                       1.0   \n",
            "3                                        0.0                                       0.0   \n",
            "4                                        0.0                                       0.0   \n",
            "5                                        0.0                                       1.0   \n",
            "6                                        0.0                                       0.0   \n",
            "7                                        0.0                                       0.0   \n",
            "8                                        0.0                                       0.0   \n",
            "9                                        0.0                                       0.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_NO_PERSON_YES_  has_entity_ORG_YES_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                        0.0                                        0.0   \n",
            "1                                        1.0                                        0.0   \n",
            "2                                        0.0                                        0.0   \n",
            "3                                        1.0                                        0.0   \n",
            "4                                        0.0                                        0.0   \n",
            "5                                        0.0                                        0.0   \n",
            "6                                        1.0                                        0.0   \n",
            "7                                        1.0                                        0.0   \n",
            "8                                        1.0                                        0.0   \n",
            "9                                        1.0                                        0.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_YES_PERSON_YES_  has_entity_nan  \n",
            "0                                         0.0             0.0  \n",
            "1                                         0.0             0.0  \n",
            "2                                         0.0             0.0  \n",
            "3                                         0.0             0.0  \n",
            "4                                         0.0             0.0  \n",
            "5                                         0.0             0.0  \n",
            "6                                         0.0             0.0  \n",
            "7                                         0.0             0.0  \n",
            "8                                         0.0             0.0  \n",
            "9                                         0.0             0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "encoder = OneHotEncoder(sparse=False, drop='first')\n",
        "X_train_encoded = encoder.fit_transform(X_train[['has_entity']])\n",
        "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(['has_entity']))\n",
        "\n",
        "X_test_encoded = encoder.transform(X_test[['has_entity']])\n",
        "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(['has_entity']))\n",
        "\n",
        "# Concatenate the encoded DataFrames with the original DataFrames\n",
        "X_train_final = pd.concat([X_train_imputed, X_train_encoded_df], axis=1)\n",
        "X_test_final = pd.concat([X_test_imputed, X_test_encoded_df], axis=1)\n",
        "print(X_train_final.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqH3BV-7zRDM",
        "outputId": "0adeea06-5053-485a-a47c-c71465c54d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           paragraph                      has_entity  \\\n",
            "0  Extension of the Bank to the north-west, the e...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "1  Thomson's separation of neon isotopes by their...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "2  The Python License was an open-source, GPL-com...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "3   Now let's be clear before we go any further t...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "4  A General System of Botany, Descriptive and An...   ORG_NO_PRODUCT_NO_PERSON_YES_   \n",
            "5  The Bahaʼi Faith asserts that evil is non-exis...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "6  In August 1993, it was discovered that the pro...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "7  Roko's basilisk has gained a significant amoun...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "8  1675 – Some Considerations about the Reconcile...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "9  For nearly 30 years Ehrenberg examined samples...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_NO_PERSON_YES_  has_entity_ORG_NO_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                       0.0                                       0.0   \n",
            "1                                       0.0                                       0.0   \n",
            "2                                       0.0                                       0.0   \n",
            "3                                       0.0                                       0.0   \n",
            "4                                       1.0                                       0.0   \n",
            "5                                       0.0                                       0.0   \n",
            "6                                       0.0                                       0.0   \n",
            "7                                       0.0                                       0.0   \n",
            "8                                       0.0                                       0.0   \n",
            "9                                       0.0                                       0.0   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_YES_PERSON_YES_  has_entity_ORG_YES_PRODUCT_NO_PERSON_NO_  \\\n",
            "0                                        0.0                                       1.0   \n",
            "1                                        0.0                                       0.0   \n",
            "2                                        0.0                                       1.0   \n",
            "3                                        0.0                                       0.0   \n",
            "4                                        0.0                                       0.0   \n",
            "5                                        0.0                                       1.0   \n",
            "6                                        0.0                                       0.0   \n",
            "7                                        0.0                                       0.0   \n",
            "8                                        0.0                                       0.0   \n",
            "9                                        0.0                                       0.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_NO_PERSON_YES_  has_entity_ORG_YES_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                        0.0                                        0.0   \n",
            "1                                        1.0                                        0.0   \n",
            "2                                        0.0                                        0.0   \n",
            "3                                        1.0                                        0.0   \n",
            "4                                        0.0                                        0.0   \n",
            "5                                        0.0                                        0.0   \n",
            "6                                        1.0                                        0.0   \n",
            "7                                        1.0                                        0.0   \n",
            "8                                        1.0                                        0.0   \n",
            "9                                        1.0                                        0.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_YES_PERSON_YES_  has_entity_nan  \\\n",
            "0                                         0.0             0.0   \n",
            "1                                         0.0             0.0   \n",
            "2                                         0.0             0.0   \n",
            "3                                         0.0             0.0   \n",
            "4                                         0.0             0.0   \n",
            "5                                         0.0             0.0   \n",
            "6                                         0.0             0.0   \n",
            "7                                         0.0             0.0   \n",
            "8                                         0.0             0.0   \n",
            "9                                         0.0             0.0   \n",
            "\n",
            "                                           tokenized  \n",
            "0  [Extension, of, the, Bank, to, the, north, -, ...  \n",
            "1  [Thomson, 's, separation, of, neon, isotopes, ...  \n",
            "2  [The, Python, License, was, an, open, -, sourc...  \n",
            "3  [ , Now, let, 's, be, clear, before, we, go, a...  \n",
            "4  [A, General, System, of, Botany, ,, Descriptiv...  \n",
            "5  [The, Bahaʼi, Faith, asserts, that, evil, is, ...  \n",
            "6  [In, August, 1993, ,, it, was, discovered, tha...  \n",
            "7  [Roko, 's, basilisk, has, gained, a, significa...  \n",
            "8  [1675, –, Some, Considerations, about, the, Re...  \n",
            "9  [For, nearly, 30, years, Ehrenberg, examined, ...  \n",
            "                                           paragraph                      has_entity  \\\n",
            "0  In 1896, FitzGerald and John Perry obtained a ...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "1   . For a more detailed derivation and more int...    ORG_NO_PRODUCT_NO_PERSON_NO_   \n",
            "2   Therefore, having minimal patient data on min...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "3  Product stewardship includes waste disposal me...    ORG_NO_PRODUCT_NO_PERSON_NO_   \n",
            "4  Recursive allocatable components – as an alter...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "5  George Gaylord Simpson  was an American paleon...   ORG_NO_PRODUCT_NO_PERSON_YES_   \n",
            "6  In some assembly languages  the same mnemonic,...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "7  that included a £40 stipend. He was also award...  ORG_YES_PRODUCT_NO_PERSON_YES_   \n",
            "8  He spent two years with Plessey Telecommunicat...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "9  In March 2000 he was awarded an honorary degre...   ORG_YES_PRODUCT_NO_PERSON_NO_   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_NO_PERSON_YES_  has_entity_ORG_NO_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                       0.0                                       0.0   \n",
            "1                                       0.0                                       0.0   \n",
            "2                                       0.0                                       0.0   \n",
            "3                                       0.0                                       0.0   \n",
            "4                                       0.0                                       0.0   \n",
            "5                                       1.0                                       0.0   \n",
            "6                                       0.0                                       0.0   \n",
            "7                                       0.0                                       0.0   \n",
            "8                                       0.0                                       0.0   \n",
            "9                                       0.0                                       0.0   \n",
            "\n",
            "   has_entity_ORG_NO_PRODUCT_YES_PERSON_YES_  has_entity_ORG_YES_PRODUCT_NO_PERSON_NO_  \\\n",
            "0                                        0.0                                       0.0   \n",
            "1                                        0.0                                       0.0   \n",
            "2                                        0.0                                       1.0   \n",
            "3                                        0.0                                       0.0   \n",
            "4                                        0.0                                       0.0   \n",
            "5                                        0.0                                       0.0   \n",
            "6                                        0.0                                       1.0   \n",
            "7                                        0.0                                       0.0   \n",
            "8                                        0.0                                       1.0   \n",
            "9                                        0.0                                       1.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_NO_PERSON_YES_  has_entity_ORG_YES_PRODUCT_YES_PERSON_NO_  \\\n",
            "0                                        1.0                                        0.0   \n",
            "1                                        0.0                                        0.0   \n",
            "2                                        0.0                                        0.0   \n",
            "3                                        0.0                                        0.0   \n",
            "4                                        1.0                                        0.0   \n",
            "5                                        0.0                                        0.0   \n",
            "6                                        0.0                                        0.0   \n",
            "7                                        1.0                                        0.0   \n",
            "8                                        0.0                                        0.0   \n",
            "9                                        0.0                                        0.0   \n",
            "\n",
            "   has_entity_ORG_YES_PRODUCT_YES_PERSON_YES_  has_entity_nan  \\\n",
            "0                                         0.0             0.0   \n",
            "1                                         0.0             0.0   \n",
            "2                                         0.0             0.0   \n",
            "3                                         0.0             0.0   \n",
            "4                                         0.0             0.0   \n",
            "5                                         0.0             0.0   \n",
            "6                                         0.0             0.0   \n",
            "7                                         0.0             0.0   \n",
            "8                                         0.0             0.0   \n",
            "9                                         0.0             0.0   \n",
            "\n",
            "                                           tokenized  \n",
            "0  [In, 1896, ,, FitzGerald, and, John, Perry, ob...  \n",
            "1  [ , ., For, a, more, detailed, derivation, and...  \n",
            "2  [ , Therefore, ,, having, minimal, patient, da...  \n",
            "3  [Product, stewardship, includes, waste, dispos...  \n",
            "4  [Recursive, allocatable, components, –, as, an...  \n",
            "5  [George, Gaylord, Simpson,  , was, an, America...  \n",
            "6  [In, some, assembly, languages,  , the, same, ...  \n",
            "7  [that, included, a, £, 40, stipend, ., He, was...  \n",
            "8  [He, spent, two, years, with, Plessey, Telecom...  \n",
            "9  [In, March, 2000, he, was, awarded, an, honora...  \n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "X_train_final['tokenized']=X_train_final['paragraph'].apply(tokenize_text)\n",
        "X_test_final['tokenized']=X_test_final['paragraph'].apply(tokenize_text)\n",
        "\n",
        "print(X_train_final.head(10))\n",
        "print(X_test_final.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEtgPqR_zX4q",
        "outputId": "4025f8d1-c9fc-4927-fb7c-19b97668177b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [extension, of, the, bank, to, the, north, -, ...\n",
            "1    [thomson, 's, separation, of, neon, isotopes, ...\n",
            "2    [the, python, license, was, an, open, -, sourc...\n",
            "3    [ , now, let, 's, be, clear, before, we, go, a...\n",
            "4    [a, general, system, of, botany, ,, descriptiv...\n",
            "5    [the, bahaʼi, faith, asserts, that, evil, is, ...\n",
            "6    [in, august, 1993, ,, it, was, discovered, tha...\n",
            "7    [roko, 's, basilisk, has, gained, a, significa...\n",
            "8    [1675, –, some, considerations, about, the, re...\n",
            "9    [for, nearly, 30, years, ehrenberg, examined, ...\n",
            "Name: tokenized, dtype: object\n",
            "0    [in, 1896, ,, fitzgerald, and, john, perry, ob...\n",
            "1    [ , ., for, a, more, detailed, derivation, and...\n",
            "2    [ , therefore, ,, having, minimal, patient, da...\n",
            "3    [product, stewardship, includes, waste, dispos...\n",
            "4    [recursive, allocatable, components, –, as, an...\n",
            "5    [george, gaylord, simpson,  , was, an, america...\n",
            "6    [in, some, assembly, languages,  , the, same, ...\n",
            "7    [that, included, a, £, 40, stipend, ., he, was...\n",
            "8    [he, spent, two, years, with, plessey, telecom...\n",
            "9    [in, march, 2000, he, was, awarded, an, honora...\n",
            "Name: tokenized, dtype: object\n"
          ]
        }
      ],
      "source": [
        "X_train_final['tokenized'] = X_train_final['tokenized'].apply(lambda tokens: [token.lower() for token in tokens])\n",
        "X_test_final['tokenized'] = X_test_final['tokenized'].apply(lambda tokens: [token.lower() for token in tokens])\n",
        "print(X_train_final['tokenized'].head(10))\n",
        "print(X_test_final['tokenized'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAaBFbhizcFt"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "def remove_stop_words(tokens):\n",
        "    doc = nlp(\" \".join(tokens))  # Join tokens into a string and process with spaCy\n",
        "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return filtered_tokens\n",
        "X_train_final['tokenized'] = X_train_final['tokenized'].apply(remove_stop_words)\n",
        "X_test_final['tokenized'] = X_test_final['tokenized'].apply(remove_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks57h_GMzf8h",
        "outputId": "453641d1-bae8-4024-faae-d1ebbfc64d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [extension, bank, north, -, west, ,, exterior,...\n",
            "1    [thomson, separation, neon, isotopes, mass, ex...\n",
            "2    [python, license, open, -, source, ,, gpl, -, ...\n",
            "3    [  , let, clear, tom, kilburn, knew, thing, co...\n",
            "4    [general, system, botany, ,, descriptive, anal...\n",
            "5    [bahaʼi, faith, asserts, evil, non, -, existen...\n",
            "6    [august, 1993, ,, discovered, proof, contained...\n",
            "7    [roko, basilisk, gained, significant, notoriet...\n",
            "8    [1675, –, considerations, reconcileableness, r...\n",
            "9    [nearly, 30, years, ehrenberg, examined, sampl...\n",
            "Name: tokenized, dtype: object\n",
            "0    [1896, ,, fitzgerald, john, perry, obtained, c...\n",
            "1    [  , ., detailed, derivation, interpretations,...\n",
            "2    [  , ,, having, minimal, patient, data, minori...\n",
            "3    [product, stewardship, includes, waste, dispos...\n",
            "4    [recursive, allocatable, components, –, altern...\n",
            "5    [george, gaylord, simpson,   , american, paleo...\n",
            "6    [assembly, languages,   , mnemonic, ,, mov, ,,...\n",
            "7    [included, £, 40, stipend, ., awarded, gold, m...\n",
            "8    [spent, years, plessey, telecommunications, lt...\n",
            "9    [march, 2000, awarded, honorary, degree, open,...\n",
            "Name: tokenized, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_train_final['tokenized'].head(10))\n",
        "print(X_test_final['tokenized'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKs7wnjUzjJc"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(tokens):\n",
        "    doc = nlp(\" \".join(tokens))  # Join tokens into a string and process with spaCy\n",
        "    filtered_tokens = [token.text for token in doc if token.text not in string.punctuation]\n",
        "    return filtered_tokens\n",
        "X_train_final['tokenized'] = X_train_final['tokenized'].apply(remove_punctuation)\n",
        "X_test_final['tokenized'] = X_test_final['tokenized'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCtThON8znVo"
      },
      "outputs": [],
      "source": [
        "print(X_train_final['tokenized'].head(10))\n",
        "print(X_test_final['tokenized'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4VfZK9DzsAO",
        "outputId": "b6c1ed22-a734-4b51-a2b6-51eed320ff1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [extension, bank, north, west, exterior, wall,...\n",
            "1    [thomson, separation, neon, isotope, mass, exa...\n",
            "2    [python, license, open, source, gpl, compatibl...\n",
            "3    [    , let, clear, tom, kilburn, know, thing, ...\n",
            "4    [general, system, botany, descriptive, analyti...\n",
            "5    [bahaʼi, faith, assert, evil, non, existent, c...\n",
            "6    [august, 1993, discover, proof, contain, flaw,...\n",
            "7    [roko, basilisk, gain, significant, notoriety,...\n",
            "8    [1675, –, consideration, reconcileableness, re...\n",
            "9    [nearly, 30, year, ehrenberg, examine, sample,...\n",
            "Name: tokenized, dtype: object\n",
            "0    [1896, fitzgerald, john, perry, obtain, civil,...\n",
            "1    [    , detailed, derivation, interpretation, e...\n",
            "2    [    , have, minimal, patient, datum, minority...\n",
            "3    [product, stewardship, include, waste, disposa...\n",
            "4    [recursive, allocatable, component, –, alterna...\n",
            "5    [george, gaylord, simpson,     , american, pal...\n",
            "6    [assembly, language,     , mnemonic, mov, fami...\n",
            "7    [include, £, 40, stipend, award, gold, medal, ...\n",
            "8    [spend, year, plessey, telecommunications, ltd...\n",
            "9    [march, 2000, award, honorary, degree, open, u...\n",
            "Name: tokenized, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def lemmatize_text(tokens):\n",
        "    doc = nlp(\" \".join(tokens))  # Join tokens into a string and process with spaCy\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    return lemmatized_tokens\n",
        "X_train_final['tokenized'] = X_train_final['tokenized'].apply(lemmatize_text)\n",
        "X_test_final['tokenized'] = X_test_final['tokenized'].apply(lemmatize_text)\n",
        "print(X_train_final['tokenized'].head(10))\n",
        "print(X_test_final['tokenized'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPWeANv4zvVg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih3Z3R0J-59Z"
      },
      "outputs": [],
      "source": [
        "print(X_train_final.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5PxCH-tzwMm"
      },
      "outputs": [],
      "source": [
        "# X_train_final['tokenized'] = X_train_tfidf.toarray()\n",
        "# X_test_final['tokenized'] = X_test_tfidf.toarray()\n",
        "# X_train_final['bigram_tokenized'] = X_train_bigram.toarray()\n",
        "# X_test_final['bigram_tokenized'] = X_test_bigram.toarray()\n",
        "# print(X_train_final.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_final.isnull().sum()"
      ],
      "metadata": {
        "id": "GuWI9imqq04b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unigram_bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))  # Specify the range for unigrams and bigrams\n",
        "X_train_unigram_bigram = unigram_bigram_vectorizer.fit_transform(X_train_final['tokenized'].apply(lambda x: ' '.join(x)))\n",
        "X_test_unigram_bigram = unigram_bigram_vectorizer.transform(X_test_final['tokenized'].apply(lambda x: ' '.join(x)))\n"
      ],
      "metadata": {
        "id": "NSGFVo0WpVXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_imputed.dtype)\n",
        "print(X_train_unigram_bigram.dtype)"
      ],
      "metadata": {
        "id": "OfEzsC9RqCIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_classifier = MultinomialNB(alpha=200.0)\n",
        "svm_classifier = SVC(C=200.0)\n",
        "\n",
        "naive_bayes_pipeline = Pipeline(steps=[('classifier', naive_bayes_classifier)])\n",
        "svm_pipeline = Pipeline(steps=[('classifier', svm_classifier)])\n",
        "\n",
        "# Define the pipelines and their corresponding names\n",
        "pipelines = [naive_bayes_pipeline, svm_pipeline]\n",
        "pipeline_names = ['Naive Bayes', 'SVM']\n",
        "\n",
        "# Iterate over each pipeline\n",
        "for pipeline, name in zip(pipelines, pipeline_names):\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train_unigram_bigram, y_train_imputed)\n",
        "\n",
        "    # Predictions on the training set\n",
        "    y_pred_train = pipeline.predict(X_train_unigram_bigram)\n",
        "    cm_train = confusion_matrix(y_train_imputed, y_pred_train)\n",
        "    print(f'{name} Training Confusion Matrix:')\n",
        "    print(cm_train)\n",
        "\n",
        "    # Predictions on the testing set\n",
        "    y_pred_test = pipeline.predict(X_test_unigram_bigram)\n",
        "    cm_test = confusion_matrix(y_test_imputed, y_pred_test)\n",
        "    print(f'{name} Testing Confusion Matrix:')\n",
        "    print(cm_test)\n",
        "\n",
        "    # Calculate overall performance\n",
        "    accuracy_train = np.mean(y_pred_train == y_train_imputed)\n",
        "    accuracy_test = np.mean(y_pred_test == y_test_imputed)\n",
        "    print(f'{name} Training Accuracy: {accuracy_train}')\n",
        "    print(f'{name} Testing Accuracy: {accuracy_test}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo5uGkR3nZVS",
        "outputId": "bf35780b-9274-4d5a-8dab-ada48610877a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Training Confusion Matrix:\n",
            "[[ 673    0    0  328   43  197]\n",
            " [   0    0    0    6    0    5]\n",
            " [   2    0    0   19    2  106]\n",
            " [   8    0    0 1935    9  133]\n",
            " [   4    0    0  120 1381   77]\n",
            " [  16    0    0  170    8 2235]]\n",
            "Naive Bayes Testing Confusion Matrix:\n",
            "[[  3   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0   0   0   2   0   0   0]\n",
            " [  0   0   3   0   0   0   0   0   0]\n",
            " [  0   0   1   2   0   0   0   0   0]\n",
            " [168   0  91  15   0  57   0   0   0]\n",
            " [  2   0  56   1   0 515   0   0   0]\n",
            " [  0   0   4   0   0  34   0   0   0]\n",
            " [  2   0 464   4   0  40   0   0   0]\n",
            " [  4   0  24 351   0  26   0   0   0]]\n",
            "Naive Bayes Training Accuracy: 0.8324194195532968\n",
            "Naive Bayes Testing Accuracy: 0.2796791443850267\n",
            "SVM Training Confusion Matrix:\n",
            "[[1231    0    0    0    0   10]\n",
            " [   0   11    0    0    0    0]\n",
            " [   0    0  128    0    0    1]\n",
            " [   0    0    0 2074    0   11]\n",
            " [   0    0    0    0 1576    6]\n",
            " [   9    0    1   13    7 2399]]\n",
            "SVM Testing Confusion Matrix:\n",
            "[[  4   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   2   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   3   0   0   0   0   0   0]\n",
            " [  0   0   0   0   3   0   0   0   0   0]\n",
            " [273   0   0  18  11   0  29   0   0   0]\n",
            " [ 13   0   0  41   6   0 514   0   0   0]\n",
            " [  1   0  25   2   1   0   9   0   0   0]\n",
            " [  6   0   0 460  16   0  28   0   0   0]\n",
            " [  8   0   0   8 363   0  26   0   0   0]]\n",
            "SVM Training Accuracy: 0.9922428781596897\n",
            "SVM Testing Accuracy: 0.2802139037433155\n"
          ]
        }
      ]
    }
  ]
}